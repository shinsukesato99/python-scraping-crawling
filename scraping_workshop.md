# Pythonã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å‹‰å¼·ä¼š

## ğŸ“‹ å‹‰å¼·ä¼šã®æ¦‚è¦

**å¯¾è±¡è€…**: Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
**æ‰€è¦æ™‚é–“**: ç´„3æ™‚é–“
**å½¢å¼**: æ¼”ç¿’ä¸­å¿ƒï¼ˆç†è«–20% / å®Ÿè·µ80%ï¼‰

---

## ğŸ¯ å­¦ç¿’ç›®æ¨™

- Pythonã‚’ä½¿ã£ãŸåŸºæœ¬çš„ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æŠ€è¡“ã®ç¿’å¾—
- requestsã€BeautifulSoupã€Seleniumã®ä½¿ã„åˆ†ã‘
- ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°æ™‚ã®ãƒãƒŠãƒ¼ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã®ç†è§£
- å®Ÿå‹™ã§ä½¿ãˆã‚‹å®Ÿè·µçš„ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¹ã‚­ãƒ«ã®ç²å¾—

---

## ğŸ›  äº‹å‰æº–å‚™

### å¿…è¦ãªç’°å¢ƒ
```bash
# Python 3.8ä»¥ä¸Š
python --version

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install requests beautifulsoup4 lxml selenium pandas
pip install playwright  # ä»»æ„
```

### æ¨å¥¨ãƒ„ãƒ¼ãƒ«
- VSCode ã¾ãŸã¯ PyCharm
- Chrome ã¾ãŸã¯ Firefox ãƒ–ãƒ©ã‚¦ã‚¶
- ChromeDriverï¼ˆSeleniumç”¨ï¼‰

---

## ğŸ“š ç¬¬1éƒ¨ï¼šã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®åŸºç¤ï¼ˆ45åˆ†ï¼‰

### 1-1. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¨ã¯

**ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆScrapingï¼‰**
- Webãƒšãƒ¼ã‚¸ã‹ã‚‰å¿…è¦ãªæƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹æŠ€è¡“
- HTML/CSSã®æ§‹é€ ã‚’è§£æã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

**ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ï¼ˆCrawlingï¼‰**
- è¤‡æ•°ã®Webãƒšãƒ¼ã‚¸ã‚’è‡ªå‹•çš„ã«å·¡å›ã™ã‚‹æŠ€è¡“
- ãƒªãƒ³ã‚¯ã‚’è¾¿ã£ã¦ãƒšãƒ¼ã‚¸ã‚’åé›†

### 1-2. æ³•çš„ãƒ»å€«ç†çš„æ³¨æ„ç‚¹

âš ï¸ **é‡è¦ãªæ³¨æ„äº‹é …**
- `robots.txt`ã‚’ç¢ºèªã—ã€éµå®ˆã™ã‚‹
- åˆ©ç”¨è¦ç´„ã‚’ç¢ºèªã™ã‚‹
- ã‚µãƒ¼ãƒãƒ¼ã«éåº¦ãªè² è·ã‚’ã‹ã‘ãªã„ï¼ˆé©åˆ‡ãªé–“éš”ã‚’ç©ºã‘ã‚‹ï¼‰
- è‘—ä½œæ¨©ãƒ»å€‹äººæƒ…å ±ã«é…æ…®ã™ã‚‹
- å•†ç”¨åˆ©ç”¨ã®å ´åˆã¯ç‰¹ã«æ³¨æ„

---

## ğŸ’» æ¼”ç¿’1ï¼šrequestsã¨BeautifulSoupã®åŸºæœ¬ï¼ˆ30åˆ†ï¼‰

### æ¼”ç¿’1-1ï¼šã‚·ãƒ³ãƒ—ãƒ«ãªHTMLå–å¾—

**èª²é¡Œ**: æŒ‡å®šã—ãŸURLã‹ã‚‰HTMLã‚’å–å¾—ã—ã¦è¡¨ç¤ºã™ã‚‹

```python
import requests
from bs4 import BeautifulSoup

# ç·´ç¿’ç”¨URLï¼ˆä¾‹ï¼šhttpbin.orgï¼‰
url = "https://httpbin.org/html"

# TODO: requestsã§HTMLã‚’å–å¾—
# TODO: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèª
# TODO: HTMLã®å†…å®¹ã‚’è¡¨ç¤º
```

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
import requests
from bs4 import BeautifulSoup

url = "https://httpbin.org/html"

response = requests.get(url)
print(f"Status Code: {response.status_code}")

if response.status_code == 200:
    print(response.text[:500])  # æœ€åˆã®500æ–‡å­—ã‚’è¡¨ç¤º
```
</details>

### æ¼”ç¿’1-2ï¼šç‰¹å®šã®è¦ç´ ã‚’æŠ½å‡º

**èª²é¡Œ**: HTMLã‹ã‚‰è¦‹å‡ºã—ï¼ˆh1, h2ï¼‰ã¨ãƒªãƒ³ã‚¯ï¼ˆaï¼‰ã‚’æŠ½å‡ºã™ã‚‹

```python
# TODO: BeautifulSoupã§HTMLã‚’ãƒ‘ãƒ¼ã‚¹
# TODO: ã™ã¹ã¦ã®h1ã‚¿ã‚°ã‚’å–å¾—
# TODO: ã™ã¹ã¦ã®aã‚¿ã‚°ã‹ã‚‰hrefå±æ€§ã‚’å–å¾—
```

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
import requests
from bs4 import BeautifulSoup

url = "https://httpbin.org/html"
response = requests.get(url)

soup = BeautifulSoup(response.text, 'html.parser')

# h1ã‚¿ã‚°ã‚’å–å¾—
h1_tags = soup.find_all('h1')
for h1 in h1_tags:
    print(f"H1: {h1.text.strip()}")

# ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
links = soup.find_all('a')
for link in links:
    href = link.get('href')
    text = link.text.strip()
    print(f"Link: {text} -> {href}")
```
</details>

### æ¼”ç¿’1-3ï¼šCSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’ä½¿ã£ãŸæŠ½å‡º

**èª²é¡Œ**: CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’ä½¿ã£ã¦ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚„IDã®è¦ç´ ã‚’å–å¾—ã™ã‚‹

```python
# TODO: ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚’æŒã¤divã‚’å–å¾—
# TODO: IDã§è¦ç´ ã‚’å–å¾—
# TODO: éšå±¤æ§‹é€ ã‚’æŒ‡å®šã—ã¦å–å¾—
```

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
import requests
from bs4 import BeautifulSoup

url = "https://example.com"  # é©åˆ‡ãªURLã«å¤‰æ›´
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# ã‚¯ãƒ©ã‚¹ã§å–å¾—
elements = soup.select('.class-name')

# IDã§å–å¾—
element = soup.select_one('#element-id')

# éšå±¤æ§‹é€ ã§å–å¾—
elements = soup.select('div.container > p.text')
```
</details>

---

## ğŸ’» æ¼”ç¿’2ï¼šå®Ÿè·µçš„ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆ45åˆ†ï¼‰

### æ¼”ç¿’2-1ï¼šãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º

**èª²é¡Œ**: Wikipediaã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦CSVã«ä¿å­˜

ç·´ç¿’ç”¨URL: `https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)`

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)"

# TODO: ãƒšãƒ¼ã‚¸ã‚’å–å¾—
# TODO: ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹
# TODO: è¡Œã¨åˆ—ã‚’è§£æ
# TODO: DataFrameã«å¤‰æ›
# TODO: CSVã«ä¿å­˜
```

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
table = soup.find('table', {'class': 'wikitable'})

# ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—
headers = []
for th in table.find_all('th'):
    headers.append(th.text.strip())

# ãƒ‡ãƒ¼ã‚¿è¡Œã‚’å–å¾—
rows = []
for tr in table.find_all('tr')[1:]:  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
    cells = tr.find_all(['td', 'th'])
    row = [cell.text.strip() for cell in cells]
    if row:
        rows.append(row)

# DataFrameã«å¤‰æ›
df = pd.DataFrame(rows, columns=headers[:len(rows[0])])
print(df.head())

# CSVã«ä¿å­˜
df.to_csv('countries_population.csv', index=False, encoding='utf-8')
```
</details>

### æ¼”ç¿’2-2ï¼šè¤‡æ•°ãƒšãƒ¼ã‚¸ã®ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°

**èª²é¡Œ**: ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹ã‚µã‚¤ãƒˆã‹ã‚‰è¤‡æ•°ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

```python
import requests
from bs4 import BeautifulSoup
import time

base_url = "https://example.com/page/"
all_data = []

# TODO: ãƒšãƒ¼ã‚¸1ã€œ5ã¾ã§é †ç•ªã«ã‚¢ã‚¯ã‚»ã‚¹
# TODO: å„ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
# TODO: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«é©åˆ‡ãªå¾…æ©Ÿæ™‚é–“ã‚’è¨­ã‘ã‚‹
# TODO: ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
```

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
import requests
from bs4 import BeautifulSoup
import time

base_url = "https://example.com/page/"
all_data = []

for page_num in range(1, 6):
    url = f"{base_url}{page_num}"
    print(f"Scraping page {page_num}...")
    
    try:
        response = requests.get(url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆå®Ÿéš›ã®æ§‹é€ ã«åˆã‚ã›ã¦å¤‰æ›´ï¼‰
        items = soup.select('.item')
        for item in items:
            data = {
                'title': item.select_one('.title').text.strip(),
                'description': item.select_one('.description').text.strip(),
            }
            all_data.append(data)
        
        # ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†å¾…æ©Ÿ
        time.sleep(2)
        
    except Exception as e:
        print(f"Error on page {page_num}: {e}")

print(f"Total items scraped: {len(all_data)}")
```
</details>

---

## ğŸ’» æ¼”ç¿’3ï¼šå‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆ40åˆ†ï¼‰

### 3-1. Seleniumã®åŸºæœ¬

JavaScriptã§å‹•çš„ã«ç”Ÿæˆã•ã‚Œã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯requests+BeautifulSoupã§ã¯å–å¾—ã§ãã¾ã›ã‚“ã€‚
Seleniumã‚’ä½¿ã†ã¨ã€å®Ÿéš›ã®ãƒ–ãƒ©ã‚¦ã‚¶ã‚’æ“ä½œã—ã¦JavaScriptå®Ÿè¡Œå¾Œã®HTMLã‚’å–å¾—ã§ãã¾ã™ã€‚

### æ¼”ç¿’3-1ï¼šSeleniumã®åŸºæœ¬æ“ä½œ

**èª²é¡Œ**: Seleniumã§ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ã—ã€ãƒšãƒ¼ã‚¸ã‚’æ“ä½œã™ã‚‹

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# TODO: WebDriverã‚’åˆæœŸåŒ–
# TODO: ãƒšãƒ¼ã‚¸ã‚’é–‹ã
# TODO: è¦ç´ ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
# TODO: è¦ç´ ã‚’å–å¾—
# TODO: ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã‚‹
```

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options

# ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
chrome_options = Options()
chrome_options.add_argument('--headless')

# WebDriverã‚’åˆæœŸåŒ–
driver = webdriver.Chrome(options=chrome_options)

try:
    # ãƒšãƒ¼ã‚¸ã‚’é–‹ã
    driver.get("https://example.com")
    
    # è¦ç´ ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿï¼ˆæœ€å¤§10ç§’ï¼‰
    wait = WebDriverWait(driver, 10)
    element = wait.until(
        EC.presence_of_element_located((By.CSS_SELECTOR, "h1"))
    )
    
    # è¦ç´ ã‚’å–å¾—
    print(element.text)
    
    # ã™ã¹ã¦ã®æ®µè½ã‚’å–å¾—
    paragraphs = driver.find_elements(By.TAG_NAME, "p")
    for p in paragraphs:
        print(p.text)
        
finally:
    # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã‚‹
    driver.quit()
```
</details>

### æ¼”ç¿’3-2ï¼šã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã¨å‹•çš„èª­ã¿è¾¼ã¿

**èª²é¡Œ**: ç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

```python
from selenium import webdriver
import time

# TODO: ãƒšãƒ¼ã‚¸ã‚’é–‹ã
# TODO: ãƒšãƒ¼ã‚¸ã®æœ€ä¸‹éƒ¨ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
# TODO: æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒèª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
# TODO: ã“ã‚Œã‚’ç¹°ã‚Šè¿”ã™
# TODO: ã™ã¹ã¦ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—
```

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
import time

driver = webdriver.Chrome()

try:
    driver.get("https://example.com/infinite-scroll")
    
    # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’ç¹°ã‚Šè¿”ã™
    last_height = driver.execute_script("return document.body.scrollHeight")
    
    for _ in range(5):  # 5å›ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
        # æœ€ä¸‹éƒ¨ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        
        # æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿ã‚’å¾…æ©Ÿ
        time.sleep(2)
        
        # æ–°ã—ã„é«˜ã•ã‚’å–å¾—
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height
    
    # ã™ã¹ã¦ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—
    items = driver.find_elements(By.CSS_SELECTOR, ".item")
    print(f"Total items: {len(items)}")
    
    for item in items:
        print(item.text)
        
finally:
    driver.quit()
```
</details>

---

## ğŸ’» æ¼”ç¿’4ï¼šã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼ˆ30åˆ†ï¼‰

### æ¼”ç¿’4-1ï¼šå …ç‰¢ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ä½œæˆ

**èª²é¡Œ**: ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å®Ÿè£…

```python
import requests
from bs4 import BeautifulSoup
import time
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

# TODO: ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥ã‚’è¨­å®š
# TODO: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
# TODO: User-Agentã‚’è¨­å®š
# TODO: ä¾‹å¤–å‡¦ç†ã‚’å®Ÿè£…
```

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
import requests
from bs4 import BeautifulSoup
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def create_session():
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
    session = requests.Session()
    
    # ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥
    retry = Retry(
        total=3,
        read=3,
        connect=3,
        backoff_factor=0.3,
        status_forcelist=(500, 502, 504)
    )
    
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    
    return session

def scrape_with_error_handling(url):
    """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    session = create_session()
    
    try:
        response = session.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        title = soup.find('h1')
        if title:
            print(f"Title: {title.text.strip()}")
        else:
            print("Title not found")
            
        return soup
        
    except requests.exceptions.HTTPError as e:
        print(f"HTTP Error: {e}")
    except requests.exceptions.ConnectionError as e:
        print(f"Connection Error: {e}")
    except requests.exceptions.Timeout as e:
        print(f"Timeout Error: {e}")
    except requests.exceptions.RequestException as e:
        print(f"Request Error: {e}")
    except Exception as e:
        print(f"Unexpected Error: {e}")
    
    return None

# å®Ÿè¡Œä¾‹
url = "https://example.com"
result = scrape_with_error_handling(url)
```
</details>

### æ¼”ç¿’4-2ï¼šãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å®Ÿè£…

**èª²é¡Œ**: é©åˆ‡ãªé–“éš”ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã™ã‚‹ä»•çµ„ã¿ã‚’å®Ÿè£…

```python
import time
from functools import wraps

# TODO: ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å®Ÿè£…
# TODO: 1ç§’ã‚ãŸã‚Šã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’åˆ¶å¾¡
```

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
import time
from functools import wraps
from datetime import datetime, timedelta

class RateLimiter:
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    def __init__(self, max_requests, time_window):
        self.max_requests = max_requests
        self.time_window = time_window  # ç§’
        self.requests = []
    
    def __call__(self, func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = datetime.now()
            
            # å¤ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆè¨˜éŒ²ã‚’å‰Šé™¤
            self.requests = [
                req_time for req_time in self.requests
                if now - req_time < timedelta(seconds=self.time_window)
            ]
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if len(self.requests) >= self.max_requests:
                sleep_time = (
                    self.requests[0] + timedelta(seconds=self.time_window) - now
                ).total_seconds()
                if sleep_time > 0:
                    print(f"Rate limit reached. Sleeping for {sleep_time:.2f}s")
                    time.sleep(sleep_time)
                    self.requests = []
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¨˜éŒ²
            self.requests.append(datetime.now())
            
            return func(*args, **kwargs)
        return wrapper

# ä½¿ç”¨ä¾‹ï¼š10ç§’é–“ã«æœ€å¤§5ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
@RateLimiter(max_requests=5, time_window=10)
def fetch_page(url):
    response = requests.get(url)
    print(f"Fetched: {url} at {datetime.now()}")
    return response

# ãƒ†ã‚¹ãƒˆ
for i in range(8):
    fetch_page(f"https://httpbin.org/delay/0")
```
</details>

---

## ğŸ“ ç¬¬5éƒ¨ï¼šå®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼ˆ30åˆ†ï¼‰

### ç·åˆæ¼”ç¿’ï¼šãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼

**èª²é¡Œ**: ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æŒã¤ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä½œæˆ

1. ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹ä¸€è¦§ã‚’å–å¾—
2. å„è¨˜äº‹ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦è©³ç´°æƒ…å ±ã‚’æŠ½å‡º
3. ãƒ‡ãƒ¼ã‚¿ã‚’JSON/CSVã§ä¿å­˜
4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ­ã‚®ãƒ³ã‚°
5. ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å®Ÿè£…

```python
import requests
from bs4 import BeautifulSoup
import json
import time
import logging
from datetime import datetime

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class NewsScraper:
    def __init__(self, base_url):
        self.base_url = base_url
        self.articles = []
        
    def scrape_article_list(self):
        """è¨˜äº‹ä¸€è¦§ã‚’å–å¾—"""
        # TODO: å®Ÿè£…
        pass
    
    def scrape_article_detail(self, article_url):
        """è¨˜äº‹è©³ç´°ã‚’å–å¾—"""
        # TODO: å®Ÿè£…
        pass
    
    def save_to_json(self, filename):
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        # TODO: å®Ÿè£…
        pass
    
    def run(self):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
        # TODO: å®Ÿè£…
        pass

# å®Ÿè¡Œ
if __name__ == "__main__":
    scraper = NewsScraper("https://example-news.com")
    scraper.run()
```

---

## ğŸ“– å‚è€ƒè³‡æ–™ãƒ»ãƒ„ãƒ¼ãƒ«

### æ¨å¥¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

- **requests**: HTTPé€šä¿¡
- **BeautifulSoup4**: HTMLãƒ‘ãƒ¼ã‚¹
- **lxml**: é«˜é€Ÿãƒ‘ãƒ¼ã‚µãƒ¼
- **Selenium**: ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–
- **Playwright**: ãƒ¢ãƒ€ãƒ³ãªãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–
- **Scrapy**: æœ¬æ ¼çš„ãªã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- **pandas**: ãƒ‡ãƒ¼ã‚¿å‡¦ç†

### ãƒ‡ãƒãƒƒã‚°ãƒ»é–‹ç™ºãƒ„ãƒ¼ãƒ«

- Chrome DevToolsï¼ˆè¦ç´ ã®æ¤œè¨¼ï¼‰
- Selector Gadgetï¼ˆCSSã‚»ãƒ¬ã‚¯ã‚¿ç”Ÿæˆï¼‰
- Postmanï¼ˆAPIãƒ†ã‚¹ãƒˆï¼‰

### å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

- [Requestså…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://requests.readthedocs.io/)
- [BeautifulSoupå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Seleniumå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://selenium-python.readthedocs.io/)

---

## âœ… ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

å‹‰å¼·ä¼šçµ‚äº†æ™‚ã«ç¢ºèªï¼š

- [ ] requestsã§åŸºæœ¬çš„ãªHTTPé€šä¿¡ãŒã§ãã‚‹
- [ ] BeautifulSoupã§è¦ç´ ã‚’æŠ½å‡ºã§ãã‚‹
- [ ] CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’ä½¿ã„ã“ãªã›ã‚‹
- [ ] è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã§ãã‚‹
- [ ] Seleniumã§å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã§ãã‚‹
- [ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒã§ãã‚‹
- [ ] ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®æ³•çš„ãƒ»å€«ç†çš„å´é¢ã‚’ç†è§£ã—ã¦ã„ã‚‹

---

## ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **Scrapyã®å­¦ç¿’**: ã‚ˆã‚Šæœ¬æ ¼çš„ãªã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
2. **éåŒæœŸå‡¦ç†**: aiohttpã€asyncioã§é«˜é€ŸåŒ–
3. **ãƒ‡ãƒ¼ã‚¿ä¿å­˜**: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆMongoDBã€PostgreSQLï¼‰ã¸ã®ä¿å­˜
4. **å®šæœŸå®Ÿè¡Œ**: cronã€Airflowã§ã®è‡ªå‹•åŒ–
5. **ã‚¯ãƒ©ã‚¦ãƒ‰åŒ–**: AWS Lambdaã€Google Cloud Functionsã§ã®é‹ç”¨

---

## â“ Q&A

ã‚ˆãã‚ã‚‹è³ªå•ã¨å›ç­”ï¼š

**Q: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯é•æ³•ã§ã™ã‹ï¼Ÿ**
A: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è‡ªä½“ã¯é•æ³•ã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€åˆ©ç”¨è¦ç´„é•åã‚„è‘—ä½œæ¨©ä¾µå®³ã«ã¯æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚

**Q: JavaScriptã§èª­ã¿è¾¼ã¾ã‚Œã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå–å¾—ã§ãã¾ã›ã‚“**
A: Seleniumã‚„ Playwrightã‚’ä½¿ç”¨ã—ã¦ãƒ–ãƒ©ã‚¦ã‚¶ã‚’ã‚¨ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã¦ãã ã•ã„ã€‚

**Q: 503ã‚¨ãƒ©ãƒ¼ãŒé »ç™ºã—ã¾ã™**
A: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®é–“éš”ã‚’ç©ºã‘ã‚‹ã€User-Agentã‚’è¨­å®šã™ã‚‹ã€robots.txtã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

---

**å‹‰å¼·ä¼šæ‹…å½“è€…**: [åå‰]
**ä½œæˆæ—¥**: 2024å¹´
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
